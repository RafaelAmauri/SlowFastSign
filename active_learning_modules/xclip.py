import os
import json
import torch

from tqdm import tqdm
from queue import PriorityQueue
from collections import OrderedDict

from active_learning_modules.xclip_utils import getKeyFramesFromFolder


def alignmentVideoGeneratedGloss(model, processor, glossesByFullVideoPath: dict, predictionsWorkDir: str, device: str) -> dict:
    """Uses the X-Clip model (https://huggingface.co/docs/transformers/v4.47.1/en/model_doc/xclip) Video-to-Text
    alignment model to check how appropriate the glosses generated by the SlowFastSign model were. That is, how 
    well they align to their corresponding video.

    Args:
        model         (transformers.XCLIPModel): The XClip model.
        processor (transformers.XCLIPProcessor): The XClip text processor.
        glossesByFullVideoPath           (dict): A dict containing the videoPaths and the corresponding glosses generated by SlowFastSign.
        unlabeledSubsetWorkDir            (str): The working directory where to store the alignment scores for each prediction.
        device                            (str): Where to run the model. CPU, Cuda, etc.

    Returns:
        dict: A dictionary containing the rankings for each gloss generated by SlowFastSign. They are ranked in lowest quality -> highest quality.
    """
    model.to(device)

    keyFrames         = []
    predictionRanking = PriorityQueue()

    loop = tqdm(glossesByFullVideoPath.items(), total=len(glossesByFullVideoPath), desc=f"Running inference for unlabeled set...")
    for vPath, predictedGloss in loop:
        # X-Clip can only handle 8 frames or 16 frames at a time, so we need to choose the 16 most significant frames :(
        keyFrames = getKeyFramesFromFolder(vPath, 16)

        # We give the model the chance to choose between the gloss that the other model generated or some random garbage.
        # If it chooses the random garbage with higher probability, it means that this could be a good candidate for labeling.
        possibleOptions = [predictedGloss, "person saying _ in sign language"]

        inputs = processor(text=possibleOptions, videos=keyFrames, return_tensors="pt", padding=True)
        inputs.to(device)

        with torch.no_grad():
            outputs = model(**inputs)

        # Get the predicted class probabilities
        probs = outputs.logits_per_video.softmax(dim=1)

        predictedLabel = possibleOptions[torch.argmax(probs)]
        predictionProbability = torch.max(probs)
        if predictedLabel == "_":
            predictionProbability = torch.tensor(0, dtype=torch.int)

        predictionRanking.put((predictionProbability, vPath))
    
    
    # Write to disk
    generatedGlossAlignmentRanking = OrderedDict()
    while not predictionRanking.empty():
        confidence, path                     = predictionRanking.get()
        confidence                           = confidence.item()
        generatedGlossAlignmentRanking[path] = confidence
    
    with open(os.path.join(predictionsWorkDir, "generatedGlossAlignmentRanking.json"), "w") as f:
        print(f"Saved to {predictionsWorkDir}/generatedGlossAlignmentRanking.json")
        json.dump(generatedGlossAlignmentRanking, f)

    return generatedGlossAlignmentRanking


def trainXClip(model, processor, nEpochs: int, dataloader, device: str):
    """Trains an X-Clip (https://huggingface.co/docs/transformers/v4.47.1/en/model_doc/xclip) Video-to-Text alignment model. 

    Args:
        model          (transformers.XCLIPModel): The XClip model.
        processor  (transformers.XCLIPProcessor): The XClip text processor.
        nEpochs                            (int): For how many epochs the model will train.
        dataloader (torch.utils.data.Dataloader): The predefined dataloader.
        device                             (str): Where to run the model. CPU, Cuda, etc.
    """
    optimizer  = torch.optim.AdamW(model.parameters(), lr=5e-6)

    model.to(device)
    model.train()

    loss_video = torch.nn.CrossEntropyLoss()
    loss_txt   = torch.nn.CrossEntropyLoss()

    for epoch in range(nEpochs):
        loop = tqdm(dataloader, total=len(dataloader), desc=f"Epoch {epoch+1}")
        for batch in loop:
            # Get the inputs

            input_ids = batch['input_ids'].squeeze(1).to(model.device)
            pixel_values = batch['pixel_values'].squeeze(1).to(model.device)

            # Forward pass
            outputs = model(input_ids=input_ids, pixel_values=pixel_values)
            
            logits_per_video = outputs.logits_per_video  # Similarity score between video and text
            logits_per_text = outputs.logits_per_text

            ground_truth = torch.arange(len(batch['input_ids']), dtype=torch.long, device=device)
            loss = (loss_video(logits_per_video,ground_truth) + loss_txt(logits_per_text,ground_truth))/2


            # Backward pass
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

    processor.save_pretrained("fine_tuned_xclip_processor")
    model.save_pretrained("fine_tuned_xclip_model")